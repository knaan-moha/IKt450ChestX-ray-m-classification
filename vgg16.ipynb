{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torchvision import  datasets, models, transforms \n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "import os \n",
    "from collections import Counter\n",
    "import torch.optim as optim\n",
    "from torchvision.models import VGG16_Weights\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, ConfusionMatrixDisplay, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    \n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  \n",
    "    transforms.RandomRotation(10), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) #* these value matches for the grayscale images\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data: 4861\n",
      "validation data: 1620\n",
      "test data: 1625\n"
     ]
    }
   ],
   "source": [
    "data_dir = './Datasets/LungXRays-grayscale'\n",
    "train_dataset = datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=transform)\n",
    "val_dataset = datasets.ImageFolder(os.path.join(data_dir, 'val'), transform=transform)\n",
    "test_dataset = datasets.ImageFolder(os.path.join(data_dir, 'test'), transform=transform)\n",
    "print(f'training data: {len(train_dataset)}')\n",
    "print(f'validation data: {len(val_dataset)}')\n",
    "print(f'test data: {len(test_dataset)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'COVID19': 1218, 'Pneumonia': 1218, 'Tuberculosis': 1218, 'Normal': 1207})\n"
     ]
    }
   ],
   "source": [
    "#TODO  check for the distribuation classes\n",
    "def get_class_distribution(dataset): \n",
    "    counter_pr_class = Counter([dataset.classes[label] for _, label in dataset.imgs])\n",
    "    return counter_pr_class   \n",
    "train_distribution = get_class_distribution(train_dataset)\n",
    "print(train_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 128, 128])\n",
      "tensor(-1.) tensor(1.)\n",
      "images shapes: torch.Size([32, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "##* datasets loader\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True); \n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "imgs, labels = next(dataiter)\n",
    "print(imgs.shape)\n",
    "print(torch.min(imgs), torch.max(imgs))\n",
    "print(f'images shapes: {imgs.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* pretrained model for vgg16 \n",
    "num_classes = len(train_dataset.classes)\n",
    "vgg16_model = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
    "vgg16_model.classifier[6] = nn.Linear(vgg16_model.classifier[6].in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = vgg16_model.to(device) \n",
    "criterion = nn.CrossEntropyLoss();  \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, )\n",
    "#optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "num_epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* we need to train the module \n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, device):\n",
    "    \n",
    "\n",
    "        #* we sting the model into train mode\n",
    "    model.train(); \n",
    "    \n",
    "    learning_loss = 0.0; \n",
    "    correct = 0; \n",
    "    total = 0; \n",
    "    \n",
    "    for inputs, labels in train_loader: \n",
    "        \n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        #* starting with the gradient from  the intial point 0 \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs); \n",
    "        loss = criterion(outputs, labels); \n",
    "        loss.backward();\n",
    "        optimizer.step(); \n",
    "        learning_loss += loss.item(); \n",
    "        _, predict = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predict == labels).sum().item(); \n",
    "    \n",
    "    learning_loss_result = learning_loss / total if total > 0 else 0\n",
    "    train_acc = 100 * correct /total if total > 0 else 0\n",
    "    return learning_loss_result, train_acc\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluation the model\n",
    "\n",
    "def validation(model, val_loader, criterion, device ): \n",
    "    model.eval(); \n",
    "    running_loss = 0.0; \n",
    "    total = 0; \n",
    "    correct = 0; \n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        for inputs, labels in val_loader: \n",
    "            inputs, labels = inputs.to(device), labels.to(device); \n",
    "            outputs = model(inputs); \n",
    "            loss = criterion(outputs, labels); \n",
    "            running_loss += loss.item(); \n",
    "            _,predict = torch.max(outputs, 1); \n",
    "            \n",
    "            total  += labels.size(0); \n",
    "            correct  += (predict == labels).sum().item()\n",
    "            \n",
    "    result_running_loss = running_loss / total if total > 0 else 0 \n",
    "    result_val_acc = 100* correct / total if total > 0 else 0; \n",
    "    return  result_running_loss, result_val_acc; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* class for the early stopping \n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience =5, delta = 0, path='best.pt'):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "    \n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss - self.delta:\n",
    "            \n",
    "            #* here we keep track for if val decreases and save the best model\n",
    "            print(f'Validation loss started decreasing from: {self.best_loss:.4f} to {val_loss:.4f} saves best model..')\n",
    "            self.best_loss = val_loss \n",
    "            self.Save_best_model(model)\n",
    "            self.counter = 0\n",
    "        else: \n",
    "            self.counter += 1 \n",
    "            print(f'Early stopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "    \n",
    "    def Save_best_model(self, model): \n",
    "        torch.save(model.state_dict(), self.path)\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Adaptive pool MPS: output sizes must be divisible by input sizes. Non-divisible input sizes are not implemented on MPS device yet. For now, you can manually transfer tensor to cpu in this case. Please refer to [this issue](https://github.com/pytorch/pytorch/issues/96056)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m train_losses, train_accs, val_losses, val_accs \u001b[38;5;241m=\u001b[39m[], [], [], []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epoch):\n\u001b[0;32m----> 6\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m train_model(model, train_loader, criterion, optimizer, device)\n\u001b[1;32m      7\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m validation(model, val_loader, criterion, device)\n\u001b[1;32m      8\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss); \n",
      "Cell \u001b[0;32mIn[19], line 18\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#* starting with the gradient from  the intial point 0 \u001b[39;00m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs); \n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels); \n\u001b[1;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward();\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/models/vgg.py:67\u001b[0m, in \u001b[0;36mVGG.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     66\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures(x)\n\u001b[0;32m---> 67\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[1;32m     68\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     69\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/pooling.py:1267\u001b[0m, in \u001b[0;36mAdaptiveAvgPool2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39madaptive_avg_pool2d(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_size)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:1260\u001b[0m, in \u001b[0;36madaptive_avg_pool2d\u001b[0;34m(input, output_size)\u001b[0m\n\u001b[1;32m   1258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(adaptive_avg_pool2d, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, output_size)\n\u001b[1;32m   1259\u001b[0m _output_size \u001b[38;5;241m=\u001b[39m _list_with_default(output_size, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 1260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39madaptive_avg_pool2d(\u001b[38;5;28minput\u001b[39m, _output_size)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Adaptive pool MPS: output sizes must be divisible by input sizes. Non-divisible input sizes are not implemented on MPS device yet. For now, you can manually transfer tensor to cpu in this case. Please refer to [this issue](https://github.com/pytorch/pytorch/issues/96056)"
     ]
    }
   ],
   "source": [
    "earlyStopping = EarlyStopping(patience=5,  path='best_vgg16.pth')\n",
    "\n",
    "train_losses, train_accs, val_losses, val_accs =[], [], [], []\n",
    "for epoch in range(num_epoch):\n",
    "    \n",
    "    train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validation(model, val_loader, criterion, device)\n",
    "    train_losses.append(train_loss); \n",
    "    train_accs.append(train_acc); \n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc); \n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epoch}: Train Loss: {train_loss:.4f} Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "    \n",
    "    earlyStopping(val_loss, model)\n",
    "    if earlyStopping.early_stop: \n",
    "        print('Note early stopping is triggered no need more training')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_model.load_state_dict(torch.load('best_vgg16.pth', map_location=device, weights_only=True))\n",
    "model = vgg16_model.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
